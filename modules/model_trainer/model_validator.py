"""
æ¨¡å‹éªŒè¯å™¨
è´Ÿè´£è®­ç»ƒå¥½çš„YOLOæ¨¡å‹çš„éªŒè¯å’Œæ€§èƒ½è¯„ä¼°
"""

import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import cv2
import numpy as np
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from modules.logger import setup_logger, LogContext

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

class ModelValidator:
    """æ¨¡å‹éªŒè¯å™¨ - è´Ÿè´£æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œæµ‹è¯•"""
    
    def __init__(self, data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ¨¡å‹éªŒè¯å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        self.data_dir = Path(data_dir)
        self.logger = setup_logger('ModelValidator')
        
        # æ£€æŸ¥ä¾èµ–
        if not ULTRALYTICS_AVAILABLE:
            self.logger.error("Ultralytics YOLOåº“æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ: pip install ultralytics")
            raise ImportError("éœ€è¦å®‰è£…ultralyticsåº“")
        
        # åŠ¨æ€åŠ è½½ç±»åˆ«æ˜ å°„ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
        self.class_names, self.chinese_names = self._load_class_mapping()
        
        self.logger.info(f"å·²åŠ è½½ {len(self.class_names)} ä¸ªç±»åˆ«")
    
    def _load_class_mapping(self) -> Tuple[Dict[int, str], Dict[int, str]]:
        """
        åŠ¨æ€åŠ è½½ç±»åˆ«æ˜ å°„ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        
        Returns:
            Tuple[Dict[int, str], Dict[int, str]]: è‹±æ–‡åç§°æ˜ å°„å’Œä¸­æ–‡åç§°æ˜ å°„
        """
        try:
            # ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„æ­£ç¡®æ˜ å°„ï¼ˆè®­ç»ƒé…ç½®æ–‡ä»¶å¯èƒ½ä¸å‡†ç¡®ï¼‰
            self.logger.info("ä½¿ç”¨ç”¨æˆ·æä¾›çš„å®é™…è®­ç»ƒæ—¶ç±»åˆ«æ˜ å°„")
            return self._get_default_mapping()
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ç±»åˆ«æ˜ å°„å¤±è´¥: {str(e)}")
            # å¦‚æœå‡ºé”™ï¼Œä»ç„¶è¿”å›æ­£ç¡®çš„æ˜ å°„
            return self._get_default_mapping()
    
    def _load_from_train_config(self, config_file: Path) -> Tuple[Dict[int, str], Dict[int, str]]:
        """ä»è®­ç»ƒé…ç½®æ–‡ä»¶åŠ è½½ç±»åˆ«æ˜ å°„"""
        import yaml
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ä»é…ç½®æ–‡ä»¶ä¸­æå–ç±»åˆ«æ˜ å°„
        class_names = config.get('names', {})
        chinese_names = {}
        
        # ä»class_detailsä¸­è·å–ä¸­æ–‡åç§°
        if 'class_details' in config and 'chinese_names' in config['class_details']:
            chinese_names = config['class_details']['chinese_names']
        else:
            # å¦‚æœæ²¡æœ‰ä¸­æ–‡åç§°ï¼Œä½¿ç”¨è‹±æ–‡åç§°
            chinese_names = class_names.copy()
        
        # ç¡®ä¿ç±»å‹æ­£ç¡®
        class_names = {int(k): str(v) for k, v in class_names.items()}
        chinese_names = {int(k): str(v) for k, v in chinese_names.items()}
        
        self.logger.info(f"ä»è®­ç»ƒé…ç½®æ–‡ä»¶åŠ è½½ç±»åˆ«æ˜ å°„: {len(class_names)} ä¸ªç±»åˆ«")
        return class_names, chinese_names
    
    def _load_from_mapping_file(self, mapping_file: Path) -> Tuple[Dict[int, str], Dict[int, str]]:
        """ä»æ˜ å°„æ–‡ä»¶åŠ è½½ç±»åˆ«"""
        class_names = {}
        chinese_names = {}
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # è§£ææ ¼å¼: "0: é’“é±¼æˆåŠŸçŠ¶æ€_txt -> fishing_success_txt"
                    if ':' in line and '->' in line:
                        parts = line.split(':')
                        class_id = int(parts[0].strip())
                        
                        name_parts = parts[1].split('->')
                        chinese_name = name_parts[0].strip()
                        english_name = name_parts[1].strip()
                        
                        class_names[class_id] = english_name
                        chinese_names[class_id] = chinese_name
        
        return class_names, chinese_names
    
    def _generate_from_data_dir(self) -> Tuple[Dict[int, str], Dict[int, str]]:
        """ä»æ•°æ®ç›®å½•åŠ¨æ€ç”Ÿæˆç±»åˆ«æ˜ å°„"""
        raw_images_dir = self.data_dir / "raw" / "images"
        
        if not raw_images_dir.exists():
            self.logger.warning(f"åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {raw_images_dir}")
            return self._get_default_mapping()
        
        # è·å–æ‰€æœ‰ç±»åˆ«ç›®å½•
        class_dirs = [d for d in raw_images_dir.iterdir() if d.is_dir()]
        class_dirs.sort()  # ç¡®ä¿é¡ºåºä¸€è‡´
        
        class_names = {}
        chinese_names = {}
        
        for idx, class_dir in enumerate(class_dirs):
            chinese_name = class_dir.name
            # ç®€å•çš„ä¸­æ–‡è½¬è‹±æ–‡è§„åˆ™
            english_name = self._chinese_to_english(chinese_name)
            
            class_names[idx] = english_name
            chinese_names[idx] = chinese_name
        
        return class_names, chinese_names
    
    def _chinese_to_english(self, chinese_name: str) -> str:
        """ç®€å•çš„ä¸­æ–‡è½¬è‹±æ–‡æ˜ å°„"""
        mapping = {
            'é’“é±¼æˆåŠŸçŠ¶æ€_txt': 'fishing_success_txt',
            'å‘å³æ‹‰_txt': 'pull_right_txt',
            'å‘å·¦æ‹‰_txt': 'pull_left_txt',
            'æçº¿ä¸­_è€åŠ›å·²åˆ°äºŒåˆ†ä¹‹ä¸€çŠ¶æ€': 'pulling_stamina_half',
            'æçº¿ä¸­_è€åŠ›æœªåˆ°äºŒåˆ†ä¹‹ä¸€çŠ¶æ€': 'pulling_stamina_low',
            'é±¼ä¸Šé’©æœ«æçº¿çŠ¶æ€': 'hooked_no_pull',
            'ç­‰å¾…ä¸Šé’©çŠ¶æ€': 'waiting_hook',
            'è¿›å…¥é’“é±¼çŠ¶æ€': 'in_fishing',
            'æœªè¿›å…¥é’“é±¼çŠ¶æ€': 'not_fishing',
            'æœ‰é±¼é¥µ': 'has_bait',
            'é€‰é¥µçŠ¶æ€': 'select_bait'
        }
        return mapping.get(chinese_name, chinese_name.lower().replace(' ', '_'))
    
    def _get_default_mapping(self) -> Tuple[Dict[int, str], Dict[int, str]]:
        """è·å–é»˜è®¤çš„ç±»åˆ«æ˜ å°„ï¼ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„æ­£ç¡®æ˜ å°„ï¼‰"""
        # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å®é™…è®­ç»ƒæ—¶æ˜ å°„
        chinese_names = {
            0: 'ç­‰å¾…ä¸Šé’©çŠ¶æ€',
            1: 'é±¼ä¸Šé’©æœ«æçº¿çŠ¶æ€',
            2: 'æçº¿ä¸­_è€åŠ›æœªåˆ°äºŒåˆ†ä¹‹ä¸€çŠ¶æ€',
            3: 'æçº¿ä¸­_è€åŠ›å·²åˆ°äºŒåˆ†ä¹‹ä¸€çŠ¶æ€',
            4: 'å‘å³æ‹‰_txt',
            5: 'å‘å·¦æ‹‰_txt',
            6: 'é’“é±¼æˆåŠŸçŠ¶æ€_txt'
        }
        
        class_names = {
            0: 'waiting_hook',
            1: 'fishing_success_txt',
            2: 'pulling_stamina_low',
            3: 'pulling_stamina_half',
            4: 'pull_right_txt',
            5: 'pull_left_txt',
            6: 'hooked_no_pull'
        }
        
        return class_names, chinese_names
    
    def validate_model(self, model_path: str) -> Dict[str, float]:
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[str, float]: éªŒè¯ç»“æœæŒ‡æ ‡
        """
        try:
            with LogContext(self.logger, f"éªŒè¯æ¨¡å‹ {model_path}"):
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
                if not Path(model_path).exists():
                    self.logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                    return {}
                
                # æ£€æŸ¥éªŒè¯æ•°æ®
                val_config = self.data_dir / "train_config.yaml"
                if not val_config.exists():
                    self.logger.error(f"éªŒè¯é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {val_config}")
                    return {}
                
                # åŠ è½½æ¨¡å‹
                model = YOLO(model_path)
                
                # æ‰§è¡ŒéªŒè¯
                self.logger.info("å¼€å§‹æ¨¡å‹éªŒè¯...")
                results = model.val(data=str(val_config), verbose=False)
                
                # æå–æŒ‡æ ‡
                metrics = self._extract_metrics(results)
                
                self.logger.info(f"æ¨¡å‹éªŒè¯å®Œæˆï¼ŒmAP@0.5: {metrics.get('map50', 0.0):.4f}")
                return metrics
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {str(e)}")
            return {}
    
    def _extract_metrics(self, results) -> Dict[str, float]:
        """
        ä»éªŒè¯ç»“æœä¸­æå–æŒ‡æ ‡
        
        Args:
            results: YOLOéªŒè¯ç»“æœ
            
        Returns:
            Dict[str, float]: æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        try:
            # ä¸»è¦æŒ‡æ ‡
            if hasattr(results, 'box'):
                box_metrics = results.box
                metrics['map50'] = float(getattr(box_metrics, 'map50', 0.0))
                metrics['map75'] = float(getattr(box_metrics, 'map75', 0.0))
                metrics['map50_95'] = float(getattr(box_metrics, 'map', 0.0))
                metrics['precision'] = float(getattr(box_metrics, 'mp', 0.0))
                metrics['recall'] = float(getattr(box_metrics, 'mr', 0.0))
            else:
                # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ç»“æœæ ¼å¼
                metrics['map50'] = float(getattr(results, 'map50', 0.0))
                metrics['map75'] = float(getattr(results, 'map75', 0.0))
                metrics['map50_95'] = float(getattr(results, 'map', 0.0))
                metrics['precision'] = float(getattr(results, 'mp', 0.0))
                metrics['recall'] = float(getattr(results, 'mr', 0.0))
            
            # è®¡ç®—F1åˆ†æ•°
            precision = metrics.get('precision', 0.0)
            recall = metrics.get('recall', 0.0)
            if precision + recall > 0:
                metrics['f1_score'] = 2 * (precision * recall) / (precision + recall)
            else:
                metrics['f1_score'] = 0.0
                
        except Exception as e:
            self.logger.warning(f"æå–æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
        
        return metrics
    
    def test_single_image(self, model_path: str, image_path: str, 
                         confidence_threshold: float = 0.5) -> Dict:
        """
        æµ‹è¯•å•å¼ å›¾ç‰‡
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            image_path: å›¾ç‰‡è·¯å¾„
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        try:
            # åŠ è½½æ¨¡å‹
            model = YOLO(model_path)
            
            # æ‰§è¡Œæ¨ç†
            results = model(image_path, conf=confidence_threshold, verbose=False)
            
            if not results:
                return {"error": "æ¨ç†å¤±è´¥"}
            
            result = results[0]
            
            # æå–æ£€æµ‹ç»“æœ
            detections = []
            if result.boxes is not None:
                for box in result.boxes:
                    # è·å–è¾¹ç•Œæ¡†åæ ‡
                    coords = box.xyxy[0].cpu().numpy()
                    
                    # è·å–ç½®ä¿¡åº¦å’Œç±»åˆ«
                    confidence = float(box.conf[0].cpu().numpy())
                    class_id = int(box.cls[0].cpu().numpy())
                    
                    detection = {
                        'bbox': coords.tolist(),
                        'confidence': confidence,
                        'class_id': class_id,
                        'class_name': self.class_names.get(class_id, f"Unknown_{class_id}"),
                        'chinese_name': self.chinese_names.get(class_id, f"æœªçŸ¥_{class_id}")
                    }
                    detections.append(detection)
            
            return {
                'image_path': image_path,
                'detections': detections,
                'detection_count': len(detections)
            }
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•å›¾ç‰‡å¤±è´¥ {image_path}: {str(e)}")
            return {"error": str(e)}
    
    def batch_test_images(self, model_path: str, image_dir: str, 
                         confidence_threshold: float = 0.5) -> List[Dict]:
        """
        æ‰¹é‡æµ‹è¯•å›¾ç‰‡
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            image_dir: å›¾ç‰‡ç›®å½•
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            List[Dict]: æ‰¹é‡æµ‹è¯•ç»“æœ
        """
        try:
            image_dir = Path(image_dir)
            if not image_dir.exists():
                self.logger.error(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {image_dir}")
                return []
            
            # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            image_files = []
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                image_files.extend(image_dir.glob(f'*{ext}'))
                image_files.extend(image_dir.glob(f'*{ext.upper()}'))
            
            if not image_files:
                self.logger.warning(f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {image_dir}")
                return []
            
            self.logger.info(f"å¼€å§‹æ‰¹é‡æµ‹è¯• {len(image_files)} å¼ å›¾ç‰‡...")
            
            results = []
            for idx, image_file in enumerate(image_files):
                self.logger.debug(f"æµ‹è¯•å›¾ç‰‡ {idx+1}/{len(image_files)}: {image_file.name}")
                
                result = self.test_single_image(
                    model_path, str(image_file), confidence_threshold
                )
                results.append(result)
            
            self.logger.info(f"æ‰¹é‡æµ‹è¯•å®Œæˆï¼Œå…±æµ‹è¯• {len(results)} å¼ å›¾ç‰‡")
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æµ‹è¯•å¤±è´¥: {str(e)}")
            return []
    
    def benchmark_model(self, model_path: str, test_images: int = 100) -> Dict:
        """
        æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            test_images: æµ‹è¯•å›¾ç‰‡æ•°é‡
            
        Returns:
            Dict: åŸºå‡†æµ‹è¯•ç»“æœ
        """
        try:
            with LogContext(self.logger, f"æ¨¡å‹åŸºå‡†æµ‹è¯• {model_path}"):
                # åŠ è½½æ¨¡å‹
                model = YOLO(model_path)
                
                # è·å–éªŒè¯é›†å›¾ç‰‡
                val_dir = self.data_dir / "val" / "images"
                if not val_dir.exists():
                    self.logger.error(f"éªŒè¯é›†ç›®å½•ä¸å­˜åœ¨: {val_dir}")
                    return {}
                
                # è·å–æµ‹è¯•å›¾ç‰‡åˆ—è¡¨
                image_files = list(val_dir.glob("*.jpg"))[:test_images]
                if not image_files:
                    self.logger.error("æ²¡æœ‰æ‰¾åˆ°éªŒè¯é›†å›¾ç‰‡")
                    return {}
                
                # æ€§èƒ½ç»Ÿè®¡
                inference_times = []
                detection_counts = []
                
                self.logger.info(f"å¼€å§‹åŸºå‡†æµ‹è¯•ï¼Œå…± {len(image_files)} å¼ å›¾ç‰‡...")
                
                for image_file in image_files:
                    # è®°å½•æ¨ç†æ—¶é—´
                    start_time = time.time()
                    
                    # æ‰§è¡Œæ¨ç†
                    results = model(str(image_file), verbose=False)
                    
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time)
                    
                    # ç»Ÿè®¡æ£€æµ‹æ•°é‡
                    if results and results[0].boxes is not None:
                        detection_count = len(results[0].boxes)
                    else:
                        detection_count = 0
                    detection_counts.append(detection_count)
                
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                avg_inference_time = np.mean(inference_times)
                min_inference_time = np.min(inference_times)
                max_inference_time = np.max(inference_times)
                fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0
                
                avg_detections = np.mean(detection_counts)
                total_detections = np.sum(detection_counts)
                
                benchmark_results = {
                    'test_images': len(image_files),
                    'avg_inference_time': avg_inference_time,
                    'min_inference_time': min_inference_time,
                    'max_inference_time': max_inference_time,
                    'fps': fps,
                    'avg_detections_per_image': avg_detections,
                    'total_detections': total_detections,
                    'detection_rate': (np.sum(np.array(detection_counts) > 0) / len(detection_counts)) * 100
                }
                
                self.logger.info(f"åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.3f}s, FPS: {fps:.1f}")
                return benchmark_results
                
        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            return {}
    
    def compare_models(self, model_paths: List[str]) -> Dict:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            model_paths: æ¨¡å‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            Dict: æ¨¡å‹æ¯”è¾ƒç»“æœ
        """
        try:
            self.logger.info(f"å¼€å§‹æ¯”è¾ƒ {len(model_paths)} ä¸ªæ¨¡å‹...")
            
            comparison_results = {}
            
            for model_path in model_paths:
                model_name = Path(model_path).stem
                self.logger.info(f"è¯„ä¼°æ¨¡å‹: {model_name}")
                
                # éªŒè¯æ¨¡å‹
                validation_metrics = self.validate_model(model_path)
                
                # åŸºå‡†æµ‹è¯•
                benchmark_results = self.benchmark_model(model_path, test_images=50)
                
                # åˆå¹¶ç»“æœ
                model_results = {
                    'validation': validation_metrics,
                    'benchmark': benchmark_results
                }
                
                comparison_results[model_name] = model_results
            
            self.logger.info("æ¨¡å‹æ¯”è¾ƒå®Œæˆ")
            return comparison_results
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹æ¯”è¾ƒå¤±è´¥: {str(e)}")
            return {}
    
    def analyze_prediction_confidence(self, model_path: str, 
                                    image_dir: str) -> Dict:
        """
        åˆ†æé¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            image_dir: å›¾ç‰‡ç›®å½•
            
        Returns:
            Dict: ç½®ä¿¡åº¦åˆ†æç»“æœ
        """
        try:
            # æ‰¹é‡æµ‹è¯•è·å–ç»“æœ
            results = self.batch_test_images(model_path, image_dir)
            
            # æ”¶é›†æ‰€æœ‰ç½®ä¿¡åº¦
            all_confidences = []
            class_confidences = {class_id: [] for class_id in self.class_names.keys()}
            
            for result in results:
                if 'detections' in result:
                    for detection in result['detections']:
                        confidence = detection['confidence']
                        class_id = detection['class_id']
                        
                        all_confidences.append(confidence)
                        if class_id in class_confidences:
                            class_confidences[class_id].append(confidence)
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            analysis = {
                'total_detections': len(all_confidences),
                'overall_confidence': {
                    'mean': float(np.mean(all_confidences)) if all_confidences else 0.0,
                    'std': float(np.std(all_confidences)) if all_confidences else 0.0,
                    'min': float(np.min(all_confidences)) if all_confidences else 0.0,
                    'max': float(np.max(all_confidences)) if all_confidences else 0.0,
                    'median': float(np.median(all_confidences)) if all_confidences else 0.0
                },
                'class_confidence': {}
            }
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            for class_id, confidences in class_confidences.items():
                if confidences:
                    class_name = self.class_names.get(class_id, f"Class_{class_id}")
                    analysis['class_confidence'][class_name] = {
                        'count': len(confidences),
                        'mean': float(np.mean(confidences)),
                        'std': float(np.std(confidences)),
                        'min': float(np.min(confidences)),
                        'max': float(np.max(confidences))
                    }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"ç½®ä¿¡åº¦åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def generate_validation_report(self, model_path: str) -> str:
        """
        ç”ŸæˆéªŒè¯æŠ¥å‘Š
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: éªŒè¯æŠ¥å‘Šæ–‡æœ¬
        """
        try:
            model_name = Path(model_path).name
            
            # è·å–å„ç§æŒ‡æ ‡
            validation_metrics = self.validate_model(model_path)
            benchmark_results = self.benchmark_model(model_path, test_images=50)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = f"""
ğŸ“Š æ¨¡å‹éªŒè¯æŠ¥å‘Š
æ¨¡å‹åç§°: {model_name}
ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ éªŒè¯æŒ‡æ ‡:
- mAP@0.5: {validation_metrics.get('map50', 0.0):.4f}
- mAP@0.75: {validation_metrics.get('map75', 0.0):.4f}
- mAP@0.5:0.95: {validation_metrics.get('map50_95', 0.0):.4f}
- ç²¾ç¡®ç‡: {validation_metrics.get('precision', 0.0):.4f}
- å¬å›ç‡: {validation_metrics.get('recall', 0.0):.4f}
- F1åˆ†æ•°: {validation_metrics.get('f1_score', 0.0):.4f}

âš¡ æ€§èƒ½åŸºå‡†:
- å¹³å‡æ¨ç†æ—¶é—´: {benchmark_results.get('avg_inference_time', 0.0):.3f}s
- FPS: {benchmark_results.get('fps', 0.0):.1f}
- æ£€æµ‹ç‡: {benchmark_results.get('detection_rate', 0.0):.1f}%
- å¹³å‡æ£€æµ‹æ•°/å›¾: {benchmark_results.get('avg_detections_per_image', 0.0):.1f}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ è¯„ä¼°æ€»ç»“:
"""
            
            # æ·»åŠ æ€§èƒ½è¯„çº§
            map50 = validation_metrics.get('map50', 0.0)
            if map50 >= 0.9:
                report += "ğŸŒŸ ä¼˜ç§€ - æ¨¡å‹æ€§èƒ½éå¸¸å¥½\n"
            elif map50 >= 0.8:
                report += "âœ… è‰¯å¥½ - æ¨¡å‹æ€§èƒ½è¾ƒå¥½\n"
            elif map50 >= 0.6:
                report += "âš ï¸ ä¸­ç­‰ - æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–\n"
            else:
                report += "âŒ è¾ƒå·® - æ¨¡å‹æ€§èƒ½ä¸è¶³ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ\n"
            
            return report
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆéªŒè¯æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}" 